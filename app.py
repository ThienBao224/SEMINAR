# =======================================================
# TR·ª¢ L√ù PH√ÇN LO·∫†I C·∫¢M X√öC TI·∫æNG VI·ªÜT
# PhoBERT fine-tuned + Dictionary + Rule ph·ªß ƒë·ªãnh + SQLite + Testcases
# =======================================================

import streamlit as st
import torch
from transformers import pipeline
import sqlite3
from datetime import datetime
import pandas as pd
import unicodedata

# =======================================================
# 1. H√ÄM B·ªé D·∫§U
# =======================================================
def remove_accents(text):
    text = unicodedata.normalize('NFD', text)
    text = text.encode('ascii', 'ignore').decode('utf-8')
    return text

# =======================================================
# 2. X·ª¨ L√ù VI·∫æT T·∫ÆT
# =======================================================
abbrev_map = {
    "ko": "kh√¥ng", "k": "kh√¥ng", "khong": "kh√¥ng", "hok": "kh√¥ng",
    "dc": "ƒë∆∞·ª£c", "dk": "ƒë∆∞·ª£c",
    "cx": "c≈©ng", "vs": "v·ªõi", "ms": "m·ªõi",
    "mik": "m√¨nh", "mk": "m√¨nh", "bn": "b·∫°n",
    "vl": "r·∫•t", "vcl": "r·∫•t",
    "okela": "ok", "oki": "ok",
    "b√πn": "bu·ªìn", "zui": "vui", "dui": "vui", "hihi": "vui", "r·∫ßu": "ch√°n", "g√©t": "gh√©t"
}

def normalize_abbrev(text):
    tokens = text.split()
    out = []
    for w in tokens:
        w_no = remove_accents(w)
        if w in abbrev_map:
            out.append(abbrev_map[w])
        elif w_no in abbrev_map:
            out.append(abbrev_map[w_no])
        else:
            out.append(w)
    return " ".join(out)

# =======================================================
# 3. TI·ªÄN X·ª¨ L√ù
# =======================================================
def preprocess(text):
    text = text.lower().strip()
    if len(text) < 2 or len(text) > 120:
        return None
    return normalize_abbrev(text)

# =======================================================
# 4. LOAD PHOBERT FINE-TUNED
# =======================================================
@st.cache_resource
def load_pipeline():
    model_name = "wonrax/phobert-base-vietnamese-sentiment"
    return pipeline("sentiment-analysis", model=model_name, tokenizer=model_name)

classifier = load_pipeline()

# =======================================================
# 5. DICTIONARY 25 T·ª™
# =======================================================
sentiment_dict = {
    "vui": "POSITIVE", "c·∫£m ∆°n": "POSITIVE", "tuy·ªát": "POSITIVE",
    "hay": "POSITIVE", "ƒë·ªânh": "POSITIVE", "th√≠ch": "POSITIVE",
    "y√™u": "POSITIVE", "h·∫°nh ph√∫c": "POSITIVE", "vui v·∫ª": "POSITIVE", "thu·∫≠n": "POSITIVE",
    "ok": "NEUTRAL", "·ªïn": "NEUTRAL", "·ªïn ƒë·ªãnh": "NEUTRAL",
    "b√¨nh th∆∞·ªùng": "NEUTRAL", "c≈©ng ƒë∆∞·ª£c": "NEUTRAL",
    "bu·ªìn": "NEGATIVE", "ch√°n": "NEGATIVE", "gh√©t": "NEGATIVE",
    "t·ªìi": "NEGATIVE", "d·ªü": "NEGATIVE", "th·∫•t v·ªçng": "NEGATIVE",
    "kh√≥ ch·ªãu": "NEGATIVE", "t·ªá": "NEGATIVE", "kh·ªßng khi·∫øp": "NEGATIVE",
    "b·ª±c m√¨nh": "NEGATIVE", "m·ªát m·ªèi": "NEGATIVE"
}

# =======================================================
# 6. MATCH DICTIONARY
# =======================================================
def dict_match(text):
    t = text.lower().strip()
    t_no = remove_accents(t)
    tokens = t.split()
    tokens_no = t_no.split()

    # C·ª•m t·ª´ 2-3 t·ª´
    for key, label in sentiment_dict.items():
        key_norm = key.lower()
        key_no = remove_accents(key_norm)
        if " " in key_norm:
            if key_norm in t or key_no in t_no:
                return label

    # T·ª´ ƒë∆°n
    for key, label in sentiment_dict.items():
        key_norm = key.lower()
        key_no = remove_accents(key_norm)
        if " " not in key_norm:
            if key_norm in tokens or key_no in tokens_no:
                return label
    return None

# =======================================================
# 7. RULE PH·ª¶ ƒê·ªäNH
# =======================================================
def negation_rule(text):
    text_low = text.lower()
    no_acc = remove_accents(text_low)
    if "khong " in no_acc or "kh√¥ng " in text_low:
        positive_words = ["vui", "vui v·∫ª", "tuy·ªát", "th√≠ch",
                          "y√™u", "h·∫°nh ph√∫c", "hay", "ƒë·ªânh", "c·∫£m ∆°n"]
        negative_words = ["bu·ªìn", "ch√°n", "gh√©t", "t·ªìi", "d·ªü",
                          "th·∫•t v·ªçng", "kh√≥ ch·ªãu", "t·ªá", "m·ªát", "m·ªát m·ªèi"]
        for w in positive_words:
            if f"khong {remove_accents(w)}" in no_acc:
                return "NEGATIVE"
        for w in negative_words:
            if f"khong {remove_accents(w)}" in no_acc:
                return "NEUTRAL"
    return None

# =======================================================
# 8. CHU·∫®N H√ìA NH√ÉN
# =======================================================
def normalize_label(label):
    label_map = {
        "POS": "POSITIVE",
        "NEG": "NEGATIVE",
        "NEU": "NEUTRAL",
        "POSITIVE": "POSITIVE",
        "NEGATIVE": "NEGATIVE",
        "NEUTRAL": "NEUTRAL"
    }
    return label_map.get(label.upper(), label.upper())

# =======================================================
# 9. PH√ÇN LO·∫†I SENTIMENT
# =======================================================
def classify_sentiment(text, threshold=0.7):
    clean = preprocess(text)
    if clean is None:
        return None, 0.0

    # Rule ph·ªß ƒë·ªãnh
    neg_label = negation_rule(clean)
    if neg_label:
        return normalize_label(neg_label), 0.98

    # Dictionary ∆∞u ti√™n
    dic_label = dict_match(clean)
    if dic_label:
        return normalize_label(dic_label), 0.99

    # PhoBERT fine-tuned
    result = classifier(clean)[0]
    label = normalize_label(result['label'])   # chu·∫©n h√≥a
    confidence = result['score']

    # C√¢u ng·∫Øn + confidence th·∫•p ‚Üí NEUTRAL
    if len(clean.split()) <= 5 and confidence < threshold:
        label = "NEUTRAL"

    return label, confidence

# =======================================================
# 10. SQLITE
# =======================================================
def init_db():
    conn = sqlite3.connect("history.db")
    conn.execute("""
        CREATE TABLE IF NOT EXISTS sentiments (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            text TEXT,
            sentiment TEXT,
            timestamp TEXT
        )
    """)
    conn.commit()
    conn.close()

def save_result(text, sentiment):
    conn = sqlite3.connect("history.db")
    timestamp = datetime.now().isoformat()
    conn.execute("INSERT INTO sentiments (text, sentiment, timestamp) VALUES (?, ?, ?)",
                 (text, sentiment, timestamp))
    conn.commit()
    conn.close()

init_db()

# =======================================================
# 11. STREAMLIT UI
# =======================================================
st.title("Tr·ª£ l√Ω ph√¢n lo·∫°i c·∫£m x√∫c ti·∫øng Vi·ªát")

text = st.text_area("Nh·∫≠p c√¢u vƒÉn:", height=100)

if st.button("Ph√¢n t√≠ch c·∫£m x√∫c"):
    sent, conf = classify_sentiment(text)
    if sent is None:
        st.error("C√¢u qu√° ng·∫Øn ho·∫∑c kh√¥ng h·ª£p l·ªá!")
    else:
        st.success(f"K·∫øt qu·∫£: **{sent}** (ƒê·ªô tin c·∫≠y: {conf*100:.1f}%)")
        save_result(text, sent)

# L·ªãch s·ª≠
if st.checkbox("Xem l·ªãch s·ª≠ (50 g·∫ßn nh·∫•t)"):
    df = pd.read_sql_query(
        "SELECT id, text, sentiment, timestamp FROM sentiments ORDER BY id DESC LIMIT 50",
        sqlite3.connect("history.db")
    )
    st.dataframe(df)

# =======================================================
# 12. TESTCASE
# =======================================================
test_cases = [
    {"text": "H√¥m nay t√¥i r·∫•t vui", "expected": "POSITIVE"},
    {"text": "M√≥n ƒÉn n√†y d·ªü qu√°", "expected": "NEGATIVE"},
    {"text": "Th·ªùi ti·∫øt b√¨nh th∆∞·ªùng", "expected": "NEUTRAL"},
    {"text": "Rat vui hom nay", "expected": "POSITIVE"},
    {"text": "C√¥ng vi·ªác ·ªïn ƒë·ªãnh", "expected": "NEUTRAL"},
    {"text": "Phim n√†y hay l·∫Øm", "expected": "POSITIVE"},
    {"text": "T√¥i bu·ªìn v√¨ th·∫•t b·∫°i", "expected": "NEGATIVE"},
    {"text": "Ng√†y mai ƒëi h·ªçc", "expected": "NEUTRAL"},
    {"text": "C·∫£m ∆°n b·∫°n r·∫•t nhi·ªÅu", "expected": "POSITIVE"},
    {"text": "M·ªát m·ªèi qu√° h√¥m nay", "expected": "NEGATIVE"},
    {"text": "Hom nay toi rat vui", "expected": "POSITIVE"},
    {"text": "Mon an nay do qua", "expected": "NEGATIVE"},
    {"text": "Thoi tiet binh thuong", "expected": "NEUTRAL"},
    {"text": "Rat vui hom nay", "expected": "POSITIVE"},
    {"text": "Cong viec on dinh", "expected": "NEUTRAL"},
    {"text": "Phim nay hay lam", "expected": "POSITIVE"},
    {"text": "Toi buon vi that bai", "expected": "NEGATIVE"},
    {"text": "Ngay mai di hoc", "expected": "NEUTRAL"},
    {"text": "Cam on ban rat nhieu", "expected": "POSITIVE"},
    {"text": "Met moi qua hom nay", "expected": "NEGATIVE"},
]

if st.sidebar.button("Ch·∫°y ki·ªÉm th·ª≠"):
    correct = 0
    results = []
    for case in test_cases:
        pred, conf = classify_sentiment(case["text"])
        pred_norm = normalize_label(pred)
        expected_norm = normalize_label(case["expected"])
        ok = (pred_norm == expected_norm)
        if ok:
            correct += 1

        results.append({
            "C√¢u": case["text"],
            "D·ª± ƒëo√°n": pred_norm,
            "ƒê·ªô tin c·∫≠y": f"{conf*100:.1f}%",
            "Mong ƒë·ª£i": expected_norm,
            "K·∫øt qu·∫£": "‚úîÔ∏è ƒê√∫ng" if ok else "‚ùå Sai"
        })

    acc = correct / len(test_cases) * 100
    st.sidebar.success(f"üéâ K·∫øt qu·∫£: {correct}/{len(test_cases)} = {acc:.1f}%")
    st.sidebar.dataframe(pd.DataFrame(results))
